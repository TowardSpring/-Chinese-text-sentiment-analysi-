{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载完成\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "import json\n",
    "\n",
    "file_name = r'D:\\文本情感分析\\wiki_word_list.json'\n",
    "with open (file_name,'rb') as f:\n",
    "    words_list = json.load(f)\n",
    "    \n",
    "file_name = r'D:\\文本情感分析\\wiki_words_vector.json'\n",
    "with open (file_name,'rb') as f:\n",
    "    words_vector = json.load(f)\n",
    "    \n",
    "print(\"数据加载完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据格式转换完成\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words_list = np.array(words_list)\n",
    "words_vector = np.array(words_vector)\n",
    "print(\"数据格式转换完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据截断完成\n"
     ]
    }
   ],
   "source": [
    "#语言模型太大了，截断，只取前10000个词\n",
    "words_list = words_list[0:10000]\n",
    "words_vector = words_vector[0:10000]\n",
    "print(\"数据截断完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论的平均长度 46.89050992165877\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#分词\n",
    "import numpy as np\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "#句子长度列表\n",
    "sentence_lens = []\n",
    "\n",
    "file_name = r'D:\\文本情感分析\\test_sentiment.txt'\n",
    "\n",
    "#读取测试集样本和标签\n",
    "with open(file_name,'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "test_sample = []    #测试集样本\n",
    "test_label = []     #测试集标签\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split()\n",
    "    test_str = line[1].decode()        #decode表示将数据解码，从bytes转换成utf-8格式\n",
    "    test_str = jieba.lcut(test_str,cut_all = False)   #对评论进行分词，以列表的形式进行保存\n",
    "    test_sample.append(test_str)    \n",
    "    test_label.append(line[2].decode())\n",
    "    sentence_lens.append(len(test_str))\n",
    "    \n",
    "#读取训练集样本和标签  \n",
    "\n",
    "file_name = r'D:\\文本情感分析\\train_sentiment.txt'\n",
    " \n",
    "with open(file_name,'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "train_sample = []    #测试集样本\n",
    "train_label = []     #测试集标签\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split()\n",
    "    train_str = line[1].decode()        #decode表示将数据解码，从bytes转换成utf-8格式\n",
    "    train_str = jieba.lcut(train_str,cut_all = False)   #对评论进行分词，以列表的形式进行保存\n",
    "    train_sample.append(train_str)\n",
    "    train_label.append(line[2].decode())\n",
    "    sentence_lens.append(len(train_str))\n",
    "    \n",
    "    \n",
    "    \n",
    "#输出句子的平均长度\n",
    "sentence_ave_len = np.mean(np.array(sentence_lens))\n",
    "print('评论的平均长度',sentence_ave_len)   #最终长度取50\n",
    "\n",
    "plt.hist(sentence_lens,50)\n",
    "plt.xlabel('Sentence length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0,120,0,2000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2ids\n",
    "max_seq_len = 60\n",
    "\n",
    "#words_list格式转换\n",
    "words_list = words_list.tolist()\n",
    "\n",
    "\n",
    "#训练数据的id转换\n",
    "ids_train = np.zeros((len(train_sample),max_seq_len),dtype='int32')\n",
    "sample_counter = 0  #下一个循环中，样本在样本集中的编号\n",
    "\n",
    "for line in train_sample:\n",
    "    index_counter = 0   #下一个循环中，单词在该单个样本中的编号\n",
    "    \n",
    "    for word in line:\n",
    "        try:\n",
    "            word_index = words_list.index(word)\n",
    "            ids_train[sample_counter][index_counter] = word_index\n",
    "            \n",
    "        except ValueError:\n",
    "            ids_train[sample_counter][index_counter] = 9999  #统一对语言模型中没有的词复制一个一样的词\n",
    "            \n",
    "        index_counter = index_counter + 1\n",
    "        \n",
    "        if index_counter >= max_seq_len:    #若一个评论中的单词书大于平均长度，则截断\n",
    "            break\n",
    "        \n",
    "    sample_counter = sample_counter + 1\n",
    "\n",
    "\n",
    "#测试数据的id转换\n",
    "ids_test = np.zeros((len(test_sample),max_seq_len),dtype='int32')\n",
    "sample_counter = 0  #下一个循环中，样本在样本集中的编号\n",
    "\n",
    "for line in test_sample:\n",
    "    index_counter = 0   #下一个循环中，单词在该单个样本中的编号\n",
    "    \n",
    "    for word in line:\n",
    "        try:\n",
    "            word_index = words_list.index(word)\n",
    "            ids_test[sample_counter][index_counter] = word_index\n",
    "            \n",
    "        except ValueError:\n",
    "            ids_test[sample_counter][index_counter] = 9999  #统一对语言模型中没有的词复制一个一样的词\n",
    "            \n",
    "        index_counter = index_counter + 1\n",
    "        \n",
    "        if index_counter >= max_seq_len:    #若一个评论中的单词书大于平均长度，则截断\n",
    "            break\n",
    "        \n",
    "    sample_counter = sample_counter + 1\n",
    "        \n",
    "#保存数据ids_train,ids_test\n",
    "file_name = r'D:\\文本情感分析\\ids_train.npy'\n",
    "np.save(file_name,ids_train)\n",
    "\n",
    "file_name = r'D:\\文本情感分析\\ids_test.npy'\n",
    "np.save(file_name,ids_test)\n",
    "\n",
    "#words_list格式转换\n",
    "words_list = np.array(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载完成\n"
     ]
    }
   ],
   "source": [
    "#加载word2ids数据（上面cell的程序跑起来比较慢，可以直接加载之前训练好的数据）\n",
    "ids_train = np.load(r'D:\\文本情感分析\\ids_train.npy')\n",
    "ids_test = np.load(r'D:\\文本情感分析\\ids_test.npy')\n",
    "print(\"数据加载完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重新设计batch数据读取\n",
    "vector_dim = 300\n",
    "#训练数据词向量整合\n",
    "total_train_data = np.zeros([len(ids_train),max_seq_len,vector_dim],dtype = np.float32)\n",
    "for i in range(len(ids_train)):\n",
    "    sentence_word_id = ids_train[i]   #每个句子的id表示向量\n",
    "    for j in range(len(sentence_word_id)):\n",
    "        solo_word_vector = words_vector[sentence_word_id[j]]  #每个句子中的一个词的词向量\n",
    "        total_train_data[i][j][:] = solo_word_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练数据特征整合\n",
    "total_train_labels = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试数据\n",
    "total_test_data = np.zeros([len(ids_test),max_seq_len,vector_dim],dtype = np.float32)\n",
    "for i in range(len(ids_test)):\n",
    "    sentence_word_id = ids_test[i]   #每个句子的id表示向量\n",
    "    for j in range(len(sentence_word_id)):\n",
    "        solo_word_vector = words_vector[sentence_word_id[j]]  #每个句子中的一个词的词向量\n",
    "        total_test_data[i][j][:] = solo_word_vector\n",
    "        \n",
    "#测试数据标签\n",
    "total_test_labels = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf.data.Dataset.from_tensor_slices（）函数的参数是tensor。\\n该函数的作用是接收tensor，对tensor的第一维度进行切分，并返回一个表示该tensor的切片数据集\\n以minist训练集为例：\\nx的shape为（60000,28,28），将x作为参数传递给tf.data.Dataset.from_tensor_slices（），\\n将返回一个含有60000个切片的数据集，每个切片为 28*28 的图像（但数据集不知道里面有多少个切片）。\\n\\nrepeat:重复此数据集count次数\\n\\nshuffle:\\n   随机混洗数据集的元素。\\n\\n   函数形式：shuffle(buffer_size,seed=None,reshuffle_each_iteration=None)\\n\\n     参数buffer_size:表示新数据集将从中采样的数据集中的元素数。\\n     参数seed:(可选）表示将用于创建分布的随机种子。\\n     参数reshuffle_each_iteration:(可选）一个布尔值，如果为true，则表示每次迭代时都应对数据集进行伪随机重组。（默认为True。）\\n     \\nbatch:\\n\\nbatch可以将数据集的连续元素合成批次。\\n\\n   函数形式：batch(batch_size,drop_remainder=False)\\n\\n   参数batch_size:表示要在单个批次中合并的此数据集的连续元素个数。\\n   参数drop_remainder：表示在少于batch_size元素的情况下是否应删除最后一批 ; 默认是不删除。\\n   \\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重新设计batch数据读取\n",
    "# Use tf.data API to shuffle and batch data.\n",
    "train_data = tf.data.Dataset.from_tensor_slices((total_train_data, total_train_labels))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数初始化完成\n"
     ]
    }
   ],
   "source": [
    "#定义模型初始化系数（超参数设定）\n",
    "\n",
    "# dataset parameters.\n",
    "num_classes = 3 # 三分类.\n",
    "#num_features = 60*300 # data features (shape: word60*vector300).\n",
    "vector_dim = 300\n",
    "max_seq_len = 60\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 60000\n",
    "batch_size = 32\n",
    "display_step = 1000\n",
    "\n",
    "num_units = 32  # number of neurons for the LSTM layer.\n",
    "\n",
    "print(\"参数初始化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实例化完成\n"
     ]
    }
   ],
   "source": [
    "#构建LSTM模型\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "class LSTM(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        # RNN (LSTM) hidden layer.\n",
    "        self.lstm_layer = layers.LSTM(units=num_units)  #,return_sequences=True)\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        # LSTM layer.\n",
    "        x = self.lstm_layer(x)\n",
    "        # Output layer (num_classes).\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # tf cross entropy expect logits without softmax, so only\n",
    "            # apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Build LSTM model.\n",
    "lstm_net = LSTM()  #实例化\n",
    "\n",
    "print(\"实例化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数、精度和优化器定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cross-Entropy Loss.\n",
    "# Note that this will apply 'softmax' to the logits.\n",
    "def cross_entropy_loss(x, y):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)   #将y的数据格式转化成dtype数据类型\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "# Adam optimizer.\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "print('损失函数、精度和优化器定义完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练流程定义完成\n"
     ]
    }
   ],
   "source": [
    "# Optimization process. 训练过程\n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        pred = lstm_net(x, is_training=True)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "        \n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = lstm_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    \n",
    "    # Update weights following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "print('训练流程定义完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):  \n",
    "    #enumerate（）枚举函数，第二个参数表示枚举计数从1开始算\n",
    "    #如果a是一个numpy array，a.take(m,1)表示取每一行的第m个值；a.take(m,0)表示取第m行\n",
    "    batch_y_int = np.empty(len(batch_y))\n",
    "    #将标签batch_y从str转换为int格式\n",
    "    for i in range(len(batch_y)):\n",
    "        temp_value = batch_y[i]\n",
    "        batch_y_int[i] = int(float(temp_value))\n",
    "    \n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization(batch_x, batch_y_int)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = lstm_net(batch_x, is_training=True)  #is_training用来设定是否边训练边验证\n",
    "        loss = cross_entropy_loss(pred, batch_y_int)\n",
    "        acc = accuracy(pred, batch_y_int)   #这里好像没有使用测试集\n",
    "        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:0.861298\n"
     ]
    }
   ],
   "source": [
    "#模型评估\n",
    "m = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_battches = int(len(ids_test)//batch_size)\n",
    "\n",
    "for batch_index in range(num_battches):\n",
    "    start_index,end_index = batch_index*batch_size,(batch_index+1)*batch_size\n",
    "    y_pred = lstm_net.predict(total_test_data[start_index:end_index])\n",
    "    batch_labels = np.empty(end_index-start_index)\n",
    "    for j in range(end_index-start_index):\n",
    "        temp_value = total_test_labels[start_index+j]\n",
    "        batch_labels[j] = int(float(temp_value))\n",
    "        \n",
    "    m.update_state(y_true = batch_labels,y_pred = y_pred)\n",
    "    \n",
    "print(\"test accuracy:%f\"%m.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "216.391px",
    "left": "1255px",
    "right": "20px",
    "top": "282px",
    "width": "749px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
